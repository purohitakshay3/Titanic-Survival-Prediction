{"cells":[{"cell_type":"code","source":["train_df = sqlContext.read.load('/FileStore/tables/train.csv', \n                          format='com.databricks.spark.csv', \n                          header='true', \n                          inferSchema='true')\n\ntest_df = sqlContext.read.load('/FileStore/tables/test.csv', \n                          format='com.databricks.spark.csv', \n                          header='true', \n                          inferSchema='true')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["## Add Survived column to test\nfrom pyspark.sql.functions import lit, col\ntrain_df = train_df.withColumn('Mark',lit('train'))\ntest_df = (test_df.withColumn('Survived',lit(0))\n                  .withColumn('Mark',lit('test')))\ntest_df = test_df[train_df.columns]\n## Append Test data to Train data\ndf = train_df.unionAll(test_df)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"Sex\", \"Embarked\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"Survived\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["cols = df.columns\n# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(df)\ndataset = pipelineModel.transform(df)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\n#display(dataset)\n#type(dataset)\ndataset.toPandas()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\", \"Fare\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nlr_value = evaluator.evaluate(predictions)\nprint lr_value"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = dtModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\", \"Fare\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\ndt_value = evaluator.evaluate(predictions)\nprint dt_value"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\", \"Fare\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nrf_value = evaluator.evaluate(predictions)\nprint rf_value"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.classification import MultilayerPerceptronClassifier\n\n# Create an initial RandomForest model.\nlayers = [4, 5, 4, 3]\nmp = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=100, layers=layers, blockSize=128, seed=1234)\n\n# Train model with Training Data\nmpModel = mp.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nmp_value = evaluator.evaluate(predictions)\nprint mp_value"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#Comparing Models\nprint(\"Linear Regresion: {}\".format(lr_value))\nprint(\"Decision Tree: {}\".format(dt_value))\nprint(\"Random Forest: {}\".format(rf_value))\nprint(\"Multilayer Perceptron: {}\".format(mp_value))"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Titanic_Akshay","notebookId":3934250680403791},"nbformat":4,"nbformat_minor":0}
